{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/karkinis/.conda/envs/ECT_CNN.cpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from itertools import starmap\n",
    "from utils import save_model, save_plots\n",
    "from CNN_execution import plot_roc_curve, ect_train_validate, report_trained_model, find_numpy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters required to define the model. \n",
    "# Will remain same throught the excerise.\n",
    " \n",
    "NUM_EPOCHS = 10 # number of epochs to train the network for; type=int\n",
    "LEARNING_RATE = 1e-3 # learning rate for training; type=float\n",
    "# loss function\n",
    "lossfcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of workers for dataloader\n",
    "num_workers = int( os.environ.get('SLURM_CPUS_PER_TASK', default=0) )\n",
    "\n",
    "# device\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 865 data for training\n"
     ]
    }
   ],
   "source": [
    "num_dirs = 4\n",
    "num_thresh = 4\n",
    "\n",
    "data_dir = '../../data'\n",
    "classes = [\n",
    "    i\n",
    "        for i in os.listdir(data_dir)\n",
    "        if os.path.isdir(os.path.join(data_dir, i))\n",
    "]\n",
    "class_items = {\n",
    "    i: find_numpy_files(os.path.join(data_dir, i))\n",
    "        for i in classes\n",
    "}\n",
    "class_items.pop('Transect')\n",
    "class_items.pop('Leafsnap')\n",
    "\n",
    "num_data_to_use_for_training = min( [len(class_items[i]) for i in class_items] )\n",
    "# num_data_to_use_for_training = 30\n",
    "print(f\"Using {num_data_to_use_for_training} data for training\")\n",
    "\n",
    "class_items = {\n",
    "    class_name: np.random.choice( file_paths, num_data_to_use_for_training, replace=False)\n",
    "        for class_name, file_paths in class_items.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ect_train_validate in module CNN_execution:\n",
      "\n",
      "ect_train_validate(num_dirs, num_thresh, input_path=None, output_ect_path='example_data/ect_output', in_memory=False, output_model_path='outputs/best_model.pth', num_epochs=50, learning_rate=0.001, lossfcn=CrossEntropyLoss(), batch_size=4, valid_split=0.2, num_workers=0, device=device(type='cpu'), recompute_ect=True, log_level='INFO')\n",
      "    Function to train and validate the CNN model using the ECT dataset.\n",
      "    Usage:\n",
      "        ect_train_validate(\n",
      "            num_dirs, num_thresh, input_path=None,\n",
      "            output_ect_path=\"example_data/ect_output\", in_memory=False,\n",
      "            output_model_path=\"outputs/best_model.pth\",\n",
      "            num_epochs=50, learning_rate=1e-3, lossfcn=nn.CrossEntropyLoss(),\n",
      "            batch_size=4, valid_split=0.2, num_workers=0,\n",
      "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
      "            recompute_ect=True, log_level='INFO'\n",
      "        )\n",
      "    Parameters:\n",
      "        num_dirs: int, number of directions for ECT calculation.\n",
      "        num_thresh: int, number of thresholds for ECT calculation.\n",
      "        input_path: str, path to the input data directory.\n",
      "        output_ect_path: str, path to save the ECT dataset. Optional, default is 'example_data/ect_output'.\n",
      "        output_model_path: str, path to save the trained model. Optional, default is 'outputs/best_model.pth'.\n",
      "        in_memory: bool, if True, the calculated ECT dataset is stored in memory and never written to files. Optional, default is False.\n",
      "        num_epochs: int, number of epochs for training. Optional, default is 50.\n",
      "        learning_rate: float, learning rate for the optimizer. Optional, default is 1e-3.\n",
      "        lossfcn: torch.nn loss function, loss function for the model.\n",
      "            Optional, default is nn.CrossEntropyLoss().\n",
      "        batch_size: int,\n",
      "            Number of data loaded per epoch for training. Optional, default is 4.\n",
      "            Higher batch size will require more memory and may speed up the training. Howerver, it may cause the model to be stuck in local minima.\n",
      "            Lower batch size will require less memory and may slow down the training. However, it may help the model to escape local minima.\n",
      "            Smaller the batch size, the more epochs are required to train the model.\n",
      "        valid_split: float,\n",
      "            Fractional ratio of data to set aside for validation. Optional, default is 0.2.\n",
      "        num_workers: int,\n",
      "            Number of workers for the data loader. Optional, default is 0.\n",
      "            Represents the number of cpu cores used for data loading.\n",
      "            This is useful for speeding up the data loading step specially when using gpu to train the model.\n",
      "            However, it requires more memory.\n",
      "        device: torch.device,\n",
      "            Device to run the model.\n",
      "        recompute_ect: bool,\n",
      "            If True, compute the ECT dataset. Optional, default is True.\n",
      "            If False, the out_path is used as the precomputed ECT dataset.\n",
      "        log_level: str or bool,\n",
      "            If True or 'INFO', print progress messages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ect_train_validate)\n",
    "\n",
    "# trained_outputs = ect_train_validate(\n",
    "#     num_dirs=num_dirs,\n",
    "#     num_thresh=num_thresh,\n",
    "#     input_path=class_items,\n",
    "#     output_ect_path='example_data/outputs',\n",
    "#     output_model_path='example_data/best_model.pth',\n",
    "#     log_level='INFO'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save_model in module utils:\n",
      "\n",
      "save_model(epochs, model, optimizer, criterion, output_model_path='outputs/best_model.pth')\n",
      "    Function to save the trained model.\n",
      "    Adapted from https://debuggercafe.com/saving-and-loading-the-best-model-in-pytorch/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(save_model)\n",
    "# save_model(\n",
    "#     epochs=trained_outputs[\"num_epochs\"],\n",
    "#     model=trained_outputs[\"model\"],\n",
    "#     optimizer=trained_outputs[\"optimizer\"],\n",
    "#     criterion=trained_outputs[\"lossfcn\"],\n",
    "#     output_model_path='example_data/best_model.pth',\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save_plots in module utils:\n",
      "\n",
      "save_plots(train_acc, valid_acc, train_loss, valid_loss, accuracy=None, loss=None, fig_size=(10, 7), dpi=300, accuracy_path='outputs/accuracy.png', loss_path='outputs/loss.png')\n",
      "    Function to save the loss and accuracy plots.\n",
      "    Usage:\n",
      "        save_plots(\n",
      "            train_acc, valid_acc, train_loss,valid_loss,\n",
      "            accuracy = None, loss = None,\n",
      "            fig_size=(10, 7), dpi=300,\n",
      "            accuracy_path = 'outputs/accuracy.png', loss_path = 'outputs/loss.png'\n",
      "        )\n",
      "    Parameters:\n",
      "        train_acc: list of training accuracy values\n",
      "        valid_acc: list of validation accuracy values\n",
      "        train_loss: list of training loss values\n",
      "        valid_loss: list of validation loss values\n",
      "        accuracy: matplotlib axis to plot accuracy. If None, a new figure is created.\n",
      "        loss: matplotlib axis to plot loss. If None, a new figure is created.\n",
      "        fig_size: tuple, size of the figure. Default is (10, 7)\n",
      "        dpi: int, resolution of the figure. Default is 300\n",
      "        accuracy_path: str, path to save the accuracy plot. Default is 'outputs/accuracy.png'\n",
      "        loss_path: str, path to save the loss plot. Default is 'outputs/loss.png'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(save_plots)\n",
    "# loss, acc = plt.figure(figsize=(9,5)).subplots(1, 2)\n",
    "# save_plots(\n",
    "#     train_acc= trained_outputs[\"train_acc\"],\n",
    "#     valid_acc= trained_outputs[\"valid_acc\"],\n",
    "#     train_loss= trained_outputs[\"train_loss\"],\n",
    "#     valid_loss= trained_outputs[\"valid_loss\"],\n",
    "#     loss=loss,\n",
    "#     accuracy=acc,\n",
    "#     accuracy_path='example_data/accuracy.png',\n",
    "#     loss_path='example_data/loss.png'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function report_trained_model in module CNN_execution:\n",
      "\n",
      "report_trained_model(num_dirs, num_thresh, train_dataset, train_loader, test_loader, test_dataset, device=device(type='cpu'), model_path='outputs/best_model.pth', ax=None, output_cf='outputs/confusion_matrix.png', output_report='outputs/outputCLFreport.csv', log_level='INFO')\n",
      "    Function to report the trained model.\n",
      "    Usage:\n",
      "        report_trained_model(\n",
      "            num_dirs, num_thresh,\n",
      "            train_dataset, train_loader, test_loader, test_dataset,\n",
      "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
      "            model_path= 'outputs/best_model.pth',\n",
      "            output_cf='outputs/confusion_matrix.png',\n",
      "            output_report='outputs/outputCLFreport.csv',\n",
      "            log_level='INFO'\n",
      "        )\n",
      "    Parameters:\n",
      "        num_dirs: int, number of directions for ECT calculation.\n",
      "        num_thresh: int, number of thresholds for ECT calculation.\n",
      "        train_dataset: torch.utils.data.Dataset, training dataset.\n",
      "        train_loader: torch.utils.data.DataLoader, training data loader.\n",
      "        test_loader: torch.utils.data.DataLoader, test data loader.\n",
      "        test_dataset: torch.utils.data.Dataset, test dataset.\n",
      "        device: torch.device, device to run the model. Optional, default is 'cuda' if available else 'cpu'.\n",
      "        model_path: str, path to the trained model. Optional, default is 'outputs/best_model.pth'.\n",
      "        ax: Axes, matplotlib figure axis to plot the confusion matrix on.\n",
      "        output_cf: str, path to save the confusion matrix plot. Optional, default is 'outputs/confusion_matrix.png'.\n",
      "        output_report: str, path to save the classification report. Optional, default is 'outputs/outputCLFreport.csv'.\n",
      "        log_level: str or bool, if True or 'INFO', print progress messages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(report_trained_model)\n",
    "# report_trained_model(\n",
    "#     num_dirs=num_dirs,\n",
    "#     num_thresh=num_thresh,\n",
    "#     train_dataset=trained_outputs[\"train_dataset\"],\n",
    "#     train_loader=trained_outputs[\"train_loader\"],\n",
    "#     test_loader=trained_outputs[\"test_loader\"],\n",
    "#     test_dataset=trained_outputs[\"test_dataset\"],\n",
    "#     model_path='example_data/best_model.pth',\n",
    "#     output_cf='example_data/confusion_matrix.png',\n",
    "#     output_report='example_data/accuracy.txt',\n",
    "#     log_level='INFO'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_roc_curve in module CNN_execution:\n",
      "\n",
      "plot_roc_curve(model, test_loader, test_dataset, device=device(type='cpu'), axis=None, output_path='outputs/roc_curve.png')\n",
      "    Function to plot the ROC curve for the trained model.\n",
      "    Usage:\n",
      "        plot_roc_curve(model, test_loader, test_dataset, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
      "    Parameters:\n",
      "        model: torch.nn model, trained model.\n",
      "        test_loader: torch.utils.data.DataLoader, test data loader.\n",
      "        test_dataset: torch.utils.data.Dataset, test dataset.\n",
      "        device: torch.device, device to run the model. Optional, default is 'cuda' if available else 'cpu'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plot_roc_curve)\n",
    "# plot_roc_curve(\n",
    "#     model=trained_outputs[\"model\"],\n",
    "#     test_loader=trained_outputs[\"test_loader\"],\n",
    "#     test_dataset=trained_outputs[\"test_dataset\"],\n",
    "#     output_path='example_data/roc_curve.png'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     directions , thresholds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i), \u001b[38;5;28mint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mect_train_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_ect_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs/output_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirections\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthresholds\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs/output_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirections\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthresholds\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     save_model(\n\u001b[1;32m     14\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mtrained_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m         model\u001b[38;5;241m=\u001b[39mtrained_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         output_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/output_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirections\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthresholds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m5\u001b[39m))\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/mnt/ufs18/rs-037/HRT841_FS24_001/ECT-Leaf-CNN/leaf-example-tutorial/CNN_execution.py:321\u001b[0m, in \u001b[0;36mect_train_validate\u001b[0;34m(num_dirs, num_thresh, input_path, output_ect_path, in_memory, output_model_path, num_epochs, learning_rate, lossfcn, batch_size, valid_split, num_workers, device, recompute_ect, log_level)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03mFunction to train and validate the CNN model using the ECT dataset.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03mUsage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m        If True or 'INFO', print progress messages.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recompute_ect:\n\u001b[0;32m--> 321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ect_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_ect_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_level\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m output_ect_path\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ufs18/rs-037/HRT841_FS24_001/ECT-Leaf-CNN/leaf-example-tutorial/CNN_execution.py:185\u001b[0m, in \u001b[0;36mgenerate_ect_dataset\u001b[0;34m(num_dirs, num_thresh, in_path, out_path, parallel, in_memory, log_level)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool() \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 185\u001b[0m         ects \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_ect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mect_arguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_memory:\n\u001b[1;32m    188\u001b[0m     data \u001b[38;5;241m=\u001b[39m {a:b \u001b[38;5;28;01mfor\u001b[39;00m a,b \u001b[38;5;129;01min\u001b[39;00m ects}\n",
      "File \u001b[0;32m~/.conda/envs/ECT_CNN.cpu/lib/python3.12/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ECT_CNN.cpu/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ECT_CNN.cpu/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ECT_CNN.cpu/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/ECT_CNN.cpu/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in 2** np.linspace(2,11,10):\n",
    "    directions , thresholds = int(i), int(i)\n",
    "    trained_model = ect_train_validate(\n",
    "        num_dirs=directions,\n",
    "        num_thresh=thresholds,\n",
    "        input_path=class_items,\n",
    "        output_ect_path=f'outputs/output_{directions}_{thresholds}/ect',\n",
    "        output_model_path=f'outputs/output_{directions}_{thresholds}/best_model.pth',\n",
    "        num_workers=num_workers,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        log_level='None'\n",
    "    )\n",
    "    save_model(\n",
    "        epochs=trained_model[\"num_epochs\"],\n",
    "        model=trained_model[\"model\"],\n",
    "        optimizer=trained_model[\"optimizer\"],\n",
    "        criterion=trained_model[\"lossfcn\"],\n",
    "        output_model_path=f'outputs/output_{directions}_{thresholds}/best_model.pth',\n",
    "    )\n",
    "    loss, acc = plt.figure(figsize=(9,5)).subplots(1, 2)\n",
    "    save_plots(\n",
    "        train_acc= trained_model[\"train_acc\"],\n",
    "        valid_acc= trained_model[\"valid_acc\"],\n",
    "        train_loss= trained_model[\"train_loss\"],\n",
    "        valid_loss= trained_model[\"valid_loss\"],\n",
    "        loss=loss,\n",
    "        accuracy=acc,\n",
    "        accuracy_path=f'outputs/output_{directions}_{thresholds}/accuracy_loss.png',\n",
    "        loss_path=f'outputs/output_{directions}_{thresholds}/accuracy_loss.png'\n",
    "    )\n",
    "    ax = plt.figure( figsize=(24,24), dpi=300 ).add_subplot(111)\n",
    "    report_trained_model(\n",
    "        num_dirs=directions,\n",
    "        num_thresh=thresholds,\n",
    "        train_dataset=trained_model[\"train_dataset\"],\n",
    "        train_loader=trained_model[\"train_loader\"],\n",
    "        test_loader=trained_model[\"test_loader\"],\n",
    "        test_dataset=trained_model[\"test_dataset\"],\n",
    "        ax=ax,\n",
    "        model_path=f'outputs/output_{directions}_{thresholds}/best_model.pth',\n",
    "        output_cf=f'outputs/output_{directions}_{thresholds}/confusion_matrix.png',\n",
    "        output_report=f'outputs/output_{directions}_{thresholds}/accuracy.txt',\n",
    "        log_level='None'\n",
    "    )\n",
    "    plot_roc_curve(\n",
    "        model=trained_model[\"model\"],\n",
    "        test_loader=trained_model[\"test_loader\"],\n",
    "        test_dataset=trained_model[\"test_dataset\"],\n",
    "        output_path=f'outputs/output_{directions}_{thresholds}/roc_curve.png'\n",
    "    )\n",
    "    print(f\"Completed training for {directions} directions and {thresholds} thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECT_CNN.cpu",
   "language": "python",
   "name": "ect_cnn.cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
