{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285d9d10",
   "metadata": {},
   "source": [
    "# Tutorial: ECT for example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34acac6-dc72-4687-8349-4c31c4df3a6f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/graphECTgif.gif\" width=\"800\" align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Only for testing purposes\n",
    "from sys import path\n",
    "path.append( \"../../ect/ect\" )\n",
    "from ect import ECT, EmbeddedGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423d195-7665-4058-a8ae-f0852eeef283",
   "metadata": {},
   "source": [
    "We are using a subset of the leaf shape dataset from [(Li et al., 2018)](https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2018.00553/full) as an example in this tutorial. Samples in the dataset are coordinates $(x,y)\\in\\mathbb{R}^2$ that form outlines of leaves. The data is pre-centered and scaled to account for any sampling resolution differences between leaf samples.\n",
    "In the published version of the dataset, these coordinates are not ordered, however in order to represent the leaf outlines as a graph, we require the coordinates be ordered such that the outline of each leaf can be traced out by the graph representation.\n",
    "To address the lack of ordering of the coordinates, we use the dataset version from [(Wiley, 2023)](https://github.com/willeyna/ECT\\_of\\_leaves), in which they use $2$-nearest neighbor graphs to order all coordinates so as to trace the outline of each leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9348e-af55-4ed6-ab2f-9ce5c2dfb786",
   "metadata": {},
   "source": [
    "Using coordinates for each leaf, ordered such that connecting subsequent coordinates in forms the leaf outline, we build a graph representation of each leaf sample. Each of the coordinates of the leaf are represented as vertices in the graph, with edges between vertices that are adjacent in the leaf outline and the graph embedding defined by the coordinates $(x,y)\\in\\mathbb{R}^2$ for each vertex. Here is an example of the graph representation for this dataset from the Cotton class, zoomed in (right, in red) to show graph structure and the subgraph highlighted in red on the full outline graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e846a1b-8204-4ebb-92e1-64e63022f15e",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/Cotton_full.png\" width=\"400\" align='center'/>\n",
    "<img src=\"images/Cotton_zoom.png\" width=\"400\" align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a77e1",
   "metadata": {},
   "source": [
    "# Load in the leaf dataset\n",
    "- This is a subset of the full dataset provided in [(Li et al., 2018)](https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2018.00553/full), with two classes (Cotton and Ivy) selected, each containing $30$ leaf outline samples.\n",
    "- The ECT of each leaf sample is computed and saved as a .npy file in a directory structure matching the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ab7cf-1781-421f-8b27-ba4b47c7eb12",
   "metadata": {},
   "source": [
    "### Here is an example loading a single leaf sample, computing the ECT, and saving the output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fdac4-813b-42e2-95bf-4ddc4937005e",
   "metadata": {},
   "source": [
    "First define the filename of the sample to be loaded and use `np.load()` to load in the numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filedir = 'example_data/outline_input/Cotton/1a_868a.npy'\n",
    "output_filedir = 'example_data/ect_output/Cotton/1a_868a.npy'\n",
    "leaf = np.load(input_filedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfad9f-ea7a-429c-9ee6-6d21c37d2ba0",
   "metadata": {},
   "source": [
    "Next, we initalize a graph object for the leaf sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = EmbeddedGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf97b0-aeb8-4494-aa4c-17975fcf0763",
   "metadata": {},
   "source": [
    "Use coordinates of the leaf outline points to add vertices (and embedding coordinates) to the graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a321994",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesX = leaf[:,0]\n",
    "valuesY = leaf[:,1]\n",
    "for i in range(np.shape(leaf)[0]):\n",
    "    G.add_node(i,valuesX[i],valuesY[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159fb7d-9e55-4445-805a-afc88ad9d419",
   "metadata": {},
   "source": [
    "Add edges to the graph according to the outline order. The samples of this dataset all have coordinates listed in order of the outline of the leaf. Note that for other datasets where this isn't the case, a different method must be used to define the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009385a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(np.shape(leaf)[0]-1):\n",
    "    G.add_edge(i, i+1)\n",
    "G.add_edge(0,np.shape(leaf)[0]-1)\n",
    "\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd911b-f434-486c-8548-bc7617d9b69a",
   "metadata": {},
   "source": [
    "Now, we initialize the ECT object using $32$ directions and $48$ thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dirs = 32\n",
    "num_thresh = 48\n",
    "\n",
    "myect = ECT(num_dirs = num_dirs, num_thresh = num_thresh)\n",
    "print(f'Directions chosen are: {myect.thetas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09890f1-7525-4f21-97c1-0cf7f37a80cc",
   "metadata": {},
   "source": [
    "Set the bounding radius to be the global dataset bounding radius, so we can use the same bounding radius for all samples of the dataset.\n",
    "<div>\n",
    "<img src=\"images/boundingbox.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "**[TO DO:]** compute the dataset global radius within ect package functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ac74e-252d-4131-9e91-62898d8435fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_bound_radius = 2.9092515639765497 \n",
    "myect.set_bounding_radius(global_bound_radius)\n",
    "print(f'Set radius is: {myect.bound_radius}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db11fd-f9e2-46d2-a7fb-e9be4f98ac81",
   "metadata": {},
   "source": [
    "Now the thresholds are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec5e31-6936-4f30-bf6e-4b0872e1110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Thresholds chosen are: {myect.threshes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373c494-a302-41d6-811e-31c5ff899590",
   "metadata": {},
   "source": [
    "Next, we calculate the ECT using the directions and thresholds specified by myect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6950532-e356-448e-a2f3-81390e230aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myect.calculateECT(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68cfb3-661d-485b-9478-90aeeec5a6fe",
   "metadata": {},
   "source": [
    "Now we can access the saved ECT matrix and optionally plot the ECT image or save the matrix as a numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babdd38b-7752-4192-a4fe-9c222a238cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The saved ECT matrix\n",
    "M = myect.get_ECT()\n",
    "print(f'M has shape: {M.shape}')\n",
    "\n",
    "myect.plot('ECT')\n",
    "\n",
    "# save the ECT matrix as a numpy file\n",
    "Path(os.path.dirname(output_filedir)).mkdir(parents=True, exist_ok=True)\n",
    "print(f'Saving the ECT of sample as: {output_filedir} ...')\n",
    "np.save(output_filedir, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fd1e9",
   "metadata": {},
   "source": [
    "### Now perform the same  process on all of the samples of the dataset, computing the ECT, and saving the output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'example_data/outline_input/'\n",
    "mypath_output = 'example_data/ect_output/'\n",
    "\n",
    "\n",
    "# loop through file system\n",
    "classes=[]\n",
    "class_count = 0\n",
    "for path, subdirs, files in os.walk(mypath):\n",
    "    classes.extend(subdirs)\n",
    "    files = [f for f in files if not f[0] == '.']\n",
    "    subdirs[:] = [d for d in subdirs if (d[0] != '.')]\n",
    "    print('Computing ECT of files in ', path, '...')\n",
    "    print(\"There are \",len(files), ' samples to load in this directory.')\n",
    "    \n",
    "    for name in files:\n",
    "        input_filedir = os.path.join(path, name)\n",
    "        leaf = np.load(input_filedir)\n",
    "        \n",
    "        # Define a graph object for the leaf\n",
    "        G = EmbeddedGraph()\n",
    "\n",
    "        # Use coordinates of the leaf outline points to add vertices (and embedding coordinates) to the graph object\n",
    "        valuesX = leaf[:,0]\n",
    "        valuesY = leaf[:,1]\n",
    "        for i in range(np.shape(leaf)[0]):\n",
    "            G.add_node(i,valuesX[i],valuesY[i])\n",
    "\n",
    "        # Add edges to the graph according to the outline order\n",
    "        for i in range(np.shape(leaf)[0]-1):\n",
    "            G.add_edge(i, i+1)\n",
    "        G.add_edge(0,np.shape(leaf)[0]-1)\n",
    "        \n",
    "        # initialize the ECT object using 32 directions and 48 thresholds\n",
    "        myect = ECT(num_dirs = num_dirs, num_thresh = num_thresh)\n",
    "\n",
    "        # Set the bounding radius to be the global dataset bounding radius\n",
    "        # TO DO: compute the dataset global radius within ect package\n",
    "        global_bound_radius = 2.9092515639765497\n",
    "        myect.set_bounding_radius(global_bound_radius)\n",
    "\n",
    "        myect.calculateECT(G)\n",
    "\n",
    "        # The saved ECT matrix\n",
    "        M = myect.get_ECT()\n",
    "\n",
    "\n",
    "        # save the ECT matrix as a numpy file\n",
    "        output_filedir = os.path.join(mypath_output+ input_filedir[len(mypath):])\n",
    "        Path(os.path.dirname(output_filedir)).mkdir(parents=True, exist_ok=True)\n",
    "        np.save(output_filedir, M)\n",
    "        \n",
    "        \n",
    "    print('Saving the ECT of directory in ', path, '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca620059",
   "metadata": {},
   "source": [
    "Now we have a saved dataset consisting of ECT matrices for the leaf outline dataset. We can train various models using these ECT matrices. Here, we show an example using a Convolutional Neural Network (CNN) to perform binary classification between the \"Cotton\" and \"Ivy\" classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d55472",
   "metadata": {},
   "source": [
    "# Train a CNN model on the ECT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742af9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7a6b-e3f8-46db-b969-5843cec4aff1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "This is the CNN model class we will use for classification, defined in `models.py`. Within this class, we specify the kernel size parameter. \n",
    "<div>\n",
    "<img src=\"images/cnn_architecture.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c0fa9-c6bb-458a-bf20-7ce8d962720d",
   "metadata": {},
   "source": [
    "Also within `models.py`, the CNN class is defined using cylinder padding scheme for both of the convolution layers. To facilitate the illusion that the input images are cylindrical, we use padding defined by built in methods as part of the PyTorch package.\n",
    "For each of the sides, where we want the left edge to be identified with the right edge of the image, we use circular padding which uses copies of columns from the right side to pad the left and copies of the left side to pad the right. On the top and bottom of each image, we pad with zero-valued pixels, which is a standard choice for computer vision tasks.\n",
    "<div>\n",
    "<img src=\"images/cylinder_and_zero_padding.jpeg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345b944-19c3-4809-8775-d290e6375164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993aacc-4616-49c0-acef-4406c9b1c8b5",
   "metadata": {},
   "source": [
    "These functions allow us to define datasets and load them into PyTorch. The dataset classes and data loaders can be adjusted in `dataloaders.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716861ad-5f11-4681-8bca-400e9349d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import create_datasets, create_data_loaders\n",
    "from utils import save_model, save_plots, save_cf, SaveBestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c254bd-48ce-4723-b238-c388e08472b9",
   "metadata": {},
   "source": [
    "Here we define a few model training parameters: the number of epochs to train the model and the learning rate. We also use a batch size of 4 samples, which is defined in `dataloaders.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa421c-533d-4c2d-bf45-57255c049964",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50 # number of epochs to train the network for; type=int\n",
    "LEARNING_RATE = 1e-3 # learning rate for training; type=float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805275f",
   "metadata": {},
   "source": [
    "### Load in the dataset and split into training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45e77c-8d98-46d6-8e92-025b81a865c0",
   "metadata": {},
   "source": [
    "Next, we build the training and test datasets as well as the dataloaders for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd859412-6ccd-4193-b8ef-526481001b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = create_datasets()\n",
    "\n",
    "train_loader, test_loader = create_data_loaders(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3304a-f936-4723-bba6-58f74e398400",
   "metadata": {},
   "source": [
    "Now, show a few samples from the training dataset. The labels for this example batch are printed as the title of each, where 0 indicates Ivy and 1 indicates Cotton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c9abd-667c-4f41-8835-92f7b79ce0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainimages, trainlabels = next(iter(train_loader))\n",
    "print(f'The tensor shape of each batch in the training dataset is \\n[batch_size, num_channels, num_directions, num_thresholds]: {trainimages.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "print('training images', trainimages.shape)\n",
    "for i in range(4):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(trainimages[i,0,:,:].T, cmap='gray')\n",
    "    label = train_dataset.get_label(trainlabels[i])\n",
    "    ax.set_title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630910b2-b3e3-4a9f-81f2-147db8976d49",
   "metadata": {},
   "source": [
    "Next, we define two functions: `train()` and `validate()`. These define the training scheme and keep track of the loss during training (and validation) as well as the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e643f0-93b9-45c1-9691-1a623d33d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, lossfcn):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        loss = lossfcn(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(train_loader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2bec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for validation\n",
    "def validate(model, valid_loader, lossfcn):\n",
    "    model.eval()\n",
    "    print('Validation')\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        outputs_list = []\n",
    "        labels_list = []\n",
    "        for i, data in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            counter += 1\n",
    "            \n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # forward pass\n",
    "            outputs = model(image)\n",
    "            outputs_list.append(outputs)\n",
    "            labels_list.append(labels)\n",
    "            # calculate the loss\n",
    "            loss = lossfcn(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # calculate the accuracy\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(valid_loader.dataset))\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca53e6-91d0-4490-8776-b4e4f8c5661d",
   "metadata": {},
   "source": [
    "Here, we specify the model to be used for training and print out a summary of the layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c07773-238b-495b-93e0-468245b6fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes=train_dataset.num_classes, num_channels=trainimages.shape[1],input_resolution=(num_dirs,num_thresh)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d19a00-dbec-4e3d-aff5-d0917e758c91",
   "metadata": {},
   "source": [
    "We can also print out the total number of parameters and trainable parameters for the model for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28fcd0-ba59-4e37-9e65-6e8f43ccd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ae904-6bc7-4f5c-b649-56b4b0d13967",
   "metadata": {},
   "source": [
    "Next, we specifify the optimizer and loss function to be used for training. In this case we use the Adam optimizer built into PyTorch, which uses a gradient descent algorithm. We use a cross entropy loss function built into PyTorch, which computes the difference between two probability distributions and outputs a score (between 0 and 1, where 0 is a perfect score) summarizing the average difference between the predicted and actual values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7ba6c-dc9b-47d0-b3cf-3400e328617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "# loss function\n",
    "lossfcn = nn.CrossEntropyLoss()\n",
    "# initialize SaveBestModel class\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96b331-fc7a-4ec6-96ba-c87e276c986c",
   "metadata": {},
   "source": [
    "Now we actually train the model using our previously defined `train()` function! During training, we keep track of overall loss and accuracy values so that we can plot the behavior of the model throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e769f-94ce-4128-a33a-bbf5b60fd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss = [],[]\n",
    "train_acc, valid_acc = [],[]\n",
    "\n",
    "# begin training\n",
    "for epoch in range(1,NUM_EPOCHS+1):\n",
    "    print(f\"[INFO]: Epoch {epoch} of {NUM_EPOCHS}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, optimizer, lossfcn)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(model, test_loader, lossfcn)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "\n",
    "    # save the best model up to current epoch, if we have the least loss in the current epoch\n",
    "    save_best_model(\n",
    "        valid_epoch_loss, epoch, model, optimizer, lossfcn\n",
    "    )\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282e251-7154-438d-93b9-038c23e09d8a",
   "metadata": {},
   "source": [
    "Finally, we save the trained model weights and the loss and accuracy plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89a9d6-33c5-4b13-b923-287371c4fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(NUM_EPOCHS, model, optimizer, lossfcn)\n",
    "save_plots(train_acc, valid_acc, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21fe26-eaf6-4b1a-b426-1d8d7d50ec2d",
   "metadata": {},
   "source": [
    "We can also visualize the model performance after training is complete thorugh a confusion matrix. We load in the best model and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5361ec1-0865-468a-a480-0c1fd3968a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, valid_loader, lossfcn\n",
    "model = CNN(num_classes=train_dataset.num_classes, num_channels=trainimages.shape[1],input_resolution=(num_dirs,num_thresh))\n",
    "\n",
    "state_dict = torch.load('outputs/best_model.pth')['model_state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print('Using validation to compute confusion matrix')\n",
    "valid_running_pred = []\n",
    "valid_running_labels = []\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        counter += 1\n",
    "        \n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        valid_running_pred.append(preds)\n",
    "        valid_running_labels.append(labels)\n",
    "    \n",
    "# confusion matrix for the complete epoch\n",
    "valid_running_pred = torch.cat(valid_running_pred)\n",
    "valid_running_labels = torch.cat(valid_running_labels)\n",
    "print('classes:',test_dataset.classes)\n",
    "save_cf(valid_running_pred.cpu(),valid_running_labels.cpu(), test_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dbf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.round(numpy.linspace(60,144.4444,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d85e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
